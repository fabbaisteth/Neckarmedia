import json
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

# 1) Load your JSON data (the file generated by your crawl script)
FILENAME = "neckarmedia_crawl.json"

with open(FILENAME, "r", encoding="utf-8") as f:
    crawled_pages = json.load(f)

# 2) Convert each record into a LangChain Document
#    so we can store metadata (url, title) alongside the content
documents = []
for page in crawled_pages:
    doc = Document(
        page_content=page["content"],  # the main text
        metadata={
            "url": page["url"],
            "title": page["title"]
        }
    )
    documents.append(doc)

# 3) Choose a text splitter
#    RecursiveCharacterTextSplitter tries to split text by sections, paragraphs, etc.
#    Adjust chunk_size and chunk_overlap to taste.
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,      # max characters per chunk
    chunk_overlap=200,    # overlap in characters between chunks
)

# 4) Split the documents into smaller chunks
chunked_docs = text_splitter.split_documents(documents)

print(f"Original docs: {len(documents)}")
print(f"After splitting: {len(chunked_docs)}")

# 5) Create embeddings
#    (Set your OpenAI API key via env var: OPENAI_API_KEY)
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
# 6) Store the chunks in a local Chroma instance
#    'persist_directory' ensures the DB is saved locally
db = Chroma.from_documents(
    documents=chunked_docs,
    embedding=embeddings,
    collection_name="neckarmedia",
    persist_directory="chroma_db"
)
db.persist()

print("Neckarmedia documents ingested into Chroma!")
